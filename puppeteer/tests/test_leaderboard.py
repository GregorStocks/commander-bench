"""Tests for leaderboard generation: Elo ratings, placement, aggregation."""

import gzip
import json
import tempfile
from pathlib import Path

from puppeteer.leaderboard import (
    _player_key,
    _split_key,
    capitalize_provider,
    compute_ratings,
    derive_display_name,
    derive_format,
    extract_placements,
    generate_all_leaderboards,
    generate_leaderboard,
    generate_leaderboard_file,
    load_model_registry,
)


def _make_game(
    game_id: str,
    timestamp: str,
    winner: str | None,
    players: list[dict],
) -> dict:
    return {
        "id": game_id,
        "timestamp": timestamp,
        "totalTurns": 10,
        "winner": winner,
        "players": players,
    }


def _pilot(
    name: str,
    model: str,
    cost: float = 1.0,
    placement: int | None = None,
    tool_calls_ok: int | None = None,
    tool_calls_failed: int | None = None,
    reasoning_effort: str | None = None,
) -> dict:
    d: dict = {"name": name, "type": "pilot", "model": model, "totalCostUsd": cost}
    if placement is not None:
        d["placement"] = placement
    if tool_calls_ok is not None:
        d["toolCallsOk"] = tool_calls_ok
    if tool_calls_failed is not None:
        d["toolCallsFailed"] = tool_calls_failed
    if reasoning_effort is not None:
        d["reasoningEffort"] = reasoning_effort
    return d


def _cpu(name: str) -> dict:
    return {"name": name, "type": "cpu", "commander": "Some Commander"}


# --- capitalize_provider ---


def test_capitalize_provider_known():
    assert capitalize_provider("anthropic") == "Anthropic"
    assert capitalize_provider("google") == "Google"
    assert capitalize_provider("openai") == "OpenAI"
    assert capitalize_provider("mistralai") == "Mistral AI"
    assert capitalize_provider("deepseek") == "DeepSeek"


def test_capitalize_provider_fallback():
    assert capitalize_provider("newprovider") == "Newprovider"


# --- derive_display_name ---


def test_derive_display_name():
    assert derive_display_name("mistralai/devstral-small") == "Devstral Small"
    assert derive_display_name("openai/gpt-4.1-mini") == "Gpt 4.1 Mini"


def test_derive_display_name_no_slash():
    assert derive_display_name("standalone") == "Standalone"


# --- load_model_registry ---


def test_load_model_registry():
    with tempfile.NamedTemporaryFile(mode="w", suffix=".json", delete=False) as f:
        json.dump(
            {
                "models": [
                    {"id": "anthropic/claude-sonnet-4.5", "name": "Claude Sonnet 4.5"},
                    {"id": "google/gemini-2.5-flash", "name": "Gemini 2.5 Flash"},
                ]
            },
            f,
        )
        path = Path(f.name)

    try:
        registry = load_model_registry(path)
        assert registry == {
            "anthropic/claude-sonnet-4.5": "Claude Sonnet 4.5",
            "google/gemini-2.5-flash": "Gemini 2.5 Flash",
        }
    finally:
        path.unlink()


def test_load_model_registry_missing_file():
    assert load_model_registry(Path("/nonexistent/models.json")) == {}


# --- extract_placements ---


def test_extract_placements_from_player_field():
    game = _make_game(
        "g1",
        "20260101_000000",
        "Alice",
        [
            _pilot("Alice", "a/x", placement=1),
            _pilot("Bob", "b/y", placement=2),
            _pilot("Carol", "c/z", placement=3),
        ],
    )
    result = extract_placements(game)
    assert result == {"Alice": 1, "Bob": 2, "Carol": 3}


def test_extract_placements_from_winner_only():
    """When no placement field and no game files, uses winner field."""
    game = _make_game(
        "g1",
        "20260101_000000",
        "Alice",
        [_pilot("Alice", "a/x"), _pilot("Bob", "b/y")],
    )
    result = extract_placements(game)
    assert result["Alice"] == 1
    assert result["Bob"] == 2


def test_extract_placements_no_winner():
    game = _make_game(
        "g1",
        "20260101_000000",
        None,
        [_pilot("Alice", "a/x"), _pilot("Bob", "b/y")],
    )
    result = extract_placements(game)
    assert result == {}


def test_extract_placements_from_game_file():
    """Falls back to reading full game JSON for elimination order."""
    with tempfile.TemporaryDirectory() as tmpdir:
        games_dir = Path(tmpdir)
        game_data = {
            "actions": [
                {"seq": 100, "message": "Carol has lost the game."},
                {"seq": 200, "message": "Bob has lost the game."},
            ]
        }
        (games_dir / "g1.json.gz").write_bytes(gzip.compress(json.dumps(game_data).encode()))

        game = _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/x"), _pilot("Bob", "b/y"), _pilot("Carol", "c/z")],
        )
        result = extract_placements(game, games_dir)
        assert result == {"Alice": 1, "Bob": 2, "Carol": 3}


# --- compute_ratings ---


def test_ratings_no_games():
    ratings, per_game = compute_ratings([])
    assert ratings == {}
    assert per_game == []


def test_ratings_winner_gains():
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/model-a", placement=1),
                _pilot("Bob", "b/model-b", placement=2),
                _pilot("Carol", "c/model-c", placement=3),
                _pilot("Dave", "d/model-d", placement=4),
            ],
        )
    ]
    ratings, _per_game = compute_ratings(games)
    # Winner should have highest rating
    assert ratings["a/model-a"] > ratings["b/model-b"]
    assert ratings["b/model-b"] > ratings["c/model-c"]
    assert ratings["c/model-c"] > ratings["d/model-d"]


def test_ratings_no_placements_no_change():
    """Games with no placement data should not change ratings."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            None,
            [_pilot("Alice", "a/model-a"), _pilot("Bob", "b/model-b")],
        )
    ]
    _ratings, per_game = compute_ratings(games)
    assert len(per_game) == 1
    # Ratings should be equal (both start the same, no update)
    assert per_game[0]["players"][0]["ratingBefore"] == per_game[0]["players"][0]["ratingAfter"]


def test_ratings_chronological_order():
    """Ratings should build up across games processed chronologically."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/x", placement=1), _pilot("Bob", "b/y", placement=2)],
        ),
        _make_game(
            "g2",
            "20260102_000000",
            "Alice",
            [_pilot("Alice", "a/x", placement=1), _pilot("Bob", "b/y", placement=2)],
        ),
    ]
    _ratings, per_game = compute_ratings(games)
    # After 2 wins, Alice should be higher than after 1 win
    assert per_game[1]["players"][0]["ratingBefore"] > per_game[0]["players"][0]["ratingBefore"]


def test_ratings_per_game_snapshots():
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/x", placement=1), _pilot("Bob", "b/y", placement=2)],
        )
    ]
    _, per_game = compute_ratings(games)
    assert len(per_game) == 1
    assert per_game[0]["id"] == "g1"
    assert len(per_game[0]["players"]) == 2

    alice = next(p for p in per_game[0]["players"] if p["key"] == "a/x")
    assert alice["ratingAfter"] > alice["ratingBefore"]

    bob = next(p for p in per_game[0]["players"] if p["key"] == "b/y")
    assert bob["ratingAfter"] < bob["ratingBefore"]


def test_ratings_skips_non_pilots():
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "CPU1",
            [_cpu("CPU1"), _pilot("Alice", "a/x")],
        )
    ]
    ratings, per_game = compute_ratings(games)
    assert "a/x" in ratings
    assert len(per_game[0]["players"]) == 1


def test_ratings_full_ordering():
    """Full ordering (1st through 4th) should produce differentiated ratings."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/a", placement=1),
                _pilot("Bob", "b/b", placement=2),
                _pilot("Carol", "c/c", placement=3),
                _pilot("Dave", "d/d", placement=4),
            ],
        )
    ]
    ratings, _ = compute_ratings(games)
    # Full ordering: each player should have a distinct rating
    sorted_by_rating = sorted(ratings.items(), key=lambda x: -x[1])
    assert sorted_by_rating[0][0] == "a/a"  # Alice won
    assert sorted_by_rating[1][0] == "b/b"  # Bob 2nd
    assert sorted_by_rating[2][0] == "c/c"  # Carol 3rd
    assert sorted_by_rating[3][0] == "d/d"  # Dave 4th


# --- generate_leaderboard ---


def test_generate_leaderboard_basic():
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/model-a", cost=5.0, placement=1),
                _pilot("Bob", "b/model-b", cost=2.0, placement=2),
            ],
        ),
        _make_game(
            "g2",
            "20260102_000000",
            "Bob",
            [
                _pilot("Alice", "a/model-a", cost=3.0, placement=2),
                _pilot("Bob", "b/model-b", cost=1.0, placement=1),
            ],
        ),
    ]
    result, ratings_by_game = generate_leaderboard(
        games,
        {},
    )
    assert result["totalGames"] == 2
    assert len(result["models"]) == 2

    # Both have 1 win in 2 games
    for m in result["models"]:
        assert m["gamesPlayed"] == 2
        assert m["winRate"] == 0.5

    assert "g1" in ratings_by_game
    assert "g2" in ratings_by_game


def test_generate_leaderboard_no_games():
    result, ratings_by_game = generate_leaderboard(
        [],
        {},
    )
    assert result["totalGames"] == 0
    assert result["models"] == []
    assert ratings_by_game == {}


def test_generate_leaderboard_no_pilots():
    games = [_make_game("g1", "20260101_000000", "CPU1", [_cpu("CPU1"), _cpu("CPU2")])]
    result, _ = generate_leaderboard(
        games,
        {},
    )
    assert result["totalGames"] == 1
    assert result["models"] == []


def test_generate_leaderboard_skips_non_pilot():
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/model-a", placement=1), _cpu("CPU1")],
        )
    ]
    result, _ = generate_leaderboard(
        games,
        {},
    )
    assert len(result["models"]) == 1
    assert result["models"][0]["modelName"] == "Model A"


def test_generate_leaderboard_uses_registry():
    registry = {"a/model-a": "Fancy Model Name"}
    games = [_make_game("g1", "20260101_000000", "Alice", [_pilot("Alice", "a/model-a", placement=1)])]
    result, _ = generate_leaderboard(
        games,
        registry,
    )
    assert result["models"][0]["modelName"] == "Fancy Model Name"


def test_generate_leaderboard_sorted_by_rating():
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/winner", placement=1),
                _pilot("Bob", "b/loser", placement=2),
            ],
        )
    ]
    result, _ = generate_leaderboard(
        games,
        {},
    )
    assert result["models"][0]["modelName"] == "Winner"
    assert result["models"][1]["modelName"] == "Loser"


def test_generate_leaderboard_missing_cost():
    player = {"name": "Alice", "type": "pilot", "model": "a/x", "placement": 1}
    games = [_make_game("g1", "20260101_000000", "Alice", [player])]
    result, _ = generate_leaderboard(
        games,
        {},
    )
    assert result["models"][0]["avgApiCost"] == 0.0


def test_generate_leaderboard_avg_cost():
    games = [
        _make_game("g1", "20260101_000000", "A", [_pilot("A", "a/x", cost=10.0, placement=1)]),
        _make_game("g2", "20260102_000000", "A", [_pilot("A", "a/x", cost=20.0, placement=1)]),
    ]
    result, _ = generate_leaderboard(
        games,
        {},
    )
    assert result["models"][0]["avgApiCost"] == 15.0


def test_generate_leaderboard_excludes_no_winner():
    """Games without a winner should be excluded from leaderboard stats."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/model-a", cost=5.0, placement=1),
                _pilot("Bob", "b/model-b", cost=2.0, placement=2),
            ],
        ),
        # No winner — should be excluded
        _make_game(
            "g2",
            "20260102_000000",
            None,
            [_pilot("Alice", "a/model-a", cost=3.0), _pilot("Bob", "b/model-b", cost=1.0)],
        ),
    ]
    result, ratings_by_game = generate_leaderboard(
        games,
        {},
    )
    assert result["totalGames"] == 1
    for m in result["models"]:
        assert m["gamesPlayed"] == 1

    alice = next(m for m in result["models"] if m["modelName"] == "Model A")
    assert alice["winRate"] == 1.0
    assert alice["avgApiCost"] == 5.0

    # No-winner game should not appear in ratings
    assert "g1" in ratings_by_game
    assert "g2" not in ratings_by_game


def test_generate_leaderboard_all_no_winner():
    """If all games lack a winner, leaderboard should be empty."""
    games = [
        _make_game("g1", "20260101_000000", None, [_pilot("A", "a/x"), _pilot("B", "b/y")]),
        _make_game("g2", "20260102_000000", None, [_pilot("A", "a/x"), _pilot("B", "b/y")]),
    ]
    result, ratings_by_game = generate_leaderboard(
        games,
        {},
    )
    assert result["totalGames"] == 0
    assert result["models"] == []
    assert ratings_by_game == {}


def test_generate_leaderboard_tool_calls():
    """Tool call counts should be averaged across games."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/model-a", placement=1, tool_calls_ok=40, tool_calls_failed=2),
                _pilot("Bob", "b/model-b", placement=2, tool_calls_ok=30, tool_calls_failed=5),
            ],
        ),
        _make_game(
            "g2",
            "20260102_000000",
            "Bob",
            [
                _pilot("Alice", "a/model-a", placement=2, tool_calls_ok=50, tool_calls_failed=4),
                _pilot("Bob", "b/model-b", placement=1, tool_calls_ok=20, tool_calls_failed=1),
            ],
        ),
    ]
    result, _ = generate_leaderboard(
        games,
        {},
    )

    alice = next(m for m in result["models"] if m["modelName"] == "Model A")
    bob = next(m for m in result["models"] if m["modelName"] == "Model B")

    # Alice: (40+50)/2 = 45.0 ok, (2+4)/2 = 3.0 failed
    assert alice["avgToolCallsOk"] == 45.0
    assert alice["avgToolCallsFailed"] == 3.0

    # Bob: (30+20)/2 = 25.0 ok, (5+1)/2 = 3.0 failed
    assert bob["avgToolCallsOk"] == 25.0
    assert bob["avgToolCallsFailed"] == 3.0


def test_generate_leaderboard_missing_tool_calls():
    """Old games without tool call fields should default to 0."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/model-a", placement=1), _pilot("Bob", "b/model-b", placement=2)],
        ),
    ]
    result, _ = generate_leaderboard(
        games,
        {},
    )

    alice = next(m for m in result["models"] if m["modelName"] == "Model A")
    assert alice["avgToolCallsOk"] == 0.0
    assert alice["avgToolCallsFailed"] == 0.0


# --- generate_leaderboard_file ---


def test_generate_leaderboard_file_integration():
    with tempfile.TemporaryDirectory() as tmpdir:
        root = Path(tmpdir)
        games_dir = root / "games"
        games_dir.mkdir()
        data_dir = root / "data"
        data_dir.mkdir()

        game = _make_game(
            "game_20260101_000000",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "anthropic/claude-sonnet-4.5", cost=5.0, placement=1),
                _pilot("Bob", "google/gemini-2.5-flash", cost=1.0, placement=2),
            ],
        )
        game["deckType"] = "Constructed - Standard"
        game["harnessEpoch"] = 3
        (games_dir / "game_20260101_000000.json.gz").write_bytes(gzip.compress(json.dumps(game).encode()))

        models_json = root / "models.json"
        models_json.write_text(
            json.dumps(
                {
                    "models": [
                        {"id": "anthropic/claude-sonnet-4.5", "name": "Claude Sonnet 4.5"},
                        {"id": "google/gemini-2.5-flash", "name": "Gemini 2.5 Flash"},
                    ]
                }
            )
        )

        output_path = generate_leaderboard_file(
            games_dir,
            data_dir,
            models_json,
        )

        # Verify benchmark-results.json
        assert output_path.exists()
        result = json.loads(output_path.read_text())
        assert result["totalGames"] == 1
        assert len(result["models"]) == 2
        assert result["models"][0]["modelName"] == "Claude Sonnet 4.5"
        assert result["models"][0]["rating"] > result["models"][1]["rating"]

        # Verify elo.json
        elo_path = games_dir.parent / "data" / "elo.json"
        assert elo_path.exists()
        elo_data = json.loads(elo_path.read_text())
        assert "game_20260101_000000" in elo_data
        assert "anthropic/claude-sonnet-4.5" in elo_data["game_20260101_000000"]
        claude_elo = elo_data["game_20260101_000000"]["anthropic/claude-sonnet-4.5"]
        assert claude_elo["after"] > claude_elo["before"]


def test_generate_leaderboard_file_no_games():
    """When no game files exist, should produce empty results."""
    with tempfile.TemporaryDirectory() as tmpdir:
        root = Path(tmpdir)
        games_dir = root / "games"
        games_dir.mkdir()
        data_dir = root / "data"

        output_path = generate_leaderboard_file(
            games_dir,
            data_dir,
            root / "models.json",
        )

        result = json.loads(output_path.read_text())
        assert result["totalGames"] == 0
        assert result["models"] == []


def test_generate_leaderboard_file_with_game_fallback():
    """When game files lack placement, reads elimination order from actions."""
    with tempfile.TemporaryDirectory() as tmpdir:
        root = Path(tmpdir)
        games_dir = root / "games"
        games_dir.mkdir()
        data_dir = root / "data"

        # Game without placement fields, but with elimination actions
        game = _make_game(
            "game_20260101_000000",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/x"), _pilot("Bob", "b/y"), _pilot("Carol", "c/z")],
        )
        game["deckType"] = "Constructed - Standard"
        game["harnessEpoch"] = 3
        game["actions"] = [
            {"seq": 100, "message": "Carol has lost the game."},
            {"seq": 200, "message": "Bob has lost the game."},
        ]
        (games_dir / "game_20260101_000000.json.gz").write_bytes(gzip.compress(json.dumps(game).encode()))

        models_json = root / "models.json"
        models_json.write_text(json.dumps({"models": []}))

        output_path = generate_leaderboard_file(
            games_dir,
            data_dir,
            models_json,
        )
        result = json.loads(output_path.read_text())

        # Alice won (1st), Bob eliminated last (2nd), Carol eliminated first (3rd)
        models_by_name = {m["modelName"]: m for m in result["models"]}
        assert models_by_name["X"]["rating"] > models_by_name["Y"]["rating"]
        assert models_by_name["Y"]["rating"] > models_by_name["Z"]["rating"]


# --- derive_format ---


def test_derive_format_legacy():
    assert derive_format({"deckType": "Constructed - Legacy"}) == "legacy"


def test_derive_format_modern():
    assert derive_format({"deckType": "Constructed - Modern"}) == "modern"


def test_derive_format_standard():
    assert derive_format({"deckType": "Constructed - Standard"}) == "standard"


def test_derive_format_commander():
    assert derive_format({"deckType": "Variant Magic - Freeform Commander"}) == "commander"


def test_derive_format_commander_default():
    """Empty deckType defaults to 'commander' for backward compat."""
    assert derive_format({}) == "commander"
    assert derive_format({"deckType": ""}) == "commander"


def test_derive_format_commander_from_game_type():
    """Commander gameType with unknown deckType -> commander."""
    assert derive_format({"gameType": "Commander Free For All", "deckType": "something"}) == "commander"


# --- generate_all_leaderboards ---


def test_generate_all_leaderboards_splits_by_format():
    legacy_game = _make_game(
        "g1",
        "20260101_000000",
        "Alice",
        [_pilot("Alice", "a/x", placement=1), _pilot("Bob", "b/y", placement=2)],
    )
    legacy_game["deckType"] = "Constructed - Legacy"

    modern_game = _make_game(
        "g2",
        "20260102_000000",
        "Carol",
        [_pilot("Carol", "c/z", placement=1), _pilot("Dave", "d/w", placement=2)],
    )
    modern_game["deckType"] = "Constructed - Modern"

    format_results, _ = generate_all_leaderboards(
        [legacy_game, modern_game],
        {},
    )

    assert "combined" in format_results
    assert "legacy" in format_results
    assert "modern" in format_results

    assert format_results["combined"]["totalGames"] == 2
    assert format_results["legacy"]["totalGames"] == 1
    assert format_results["modern"]["totalGames"] == 1


def test_generate_all_leaderboards_combined_includes_all():
    games = []
    for i, fmt in enumerate(["Constructed - Legacy", "Constructed - Modern", "Constructed - Standard"]):
        g = _make_game(
            f"g{i}",
            f"2026010{i}_000000",
            "Alice",
            [_pilot("Alice", "a/x", placement=1), _pilot("Bob", "b/y", placement=2)],
        )
        g["deckType"] = fmt
        games.append(g)

    format_results, _ = generate_all_leaderboards(
        games,
        {},
    )
    assert format_results["combined"]["totalGames"] == 3


def test_generate_leaderboard_file_has_formats_key():
    """benchmark-results.json should have top-level fields AND formats key."""
    with tempfile.TemporaryDirectory() as tmpdir:
        root = Path(tmpdir)
        games_dir = root / "games"
        games_dir.mkdir()
        data_dir = root / "data"

        game = _make_game(
            "game_20260101_000000",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/x", cost=5.0, placement=1), _pilot("Bob", "b/y", cost=2.0, placement=2)],
        )
        game["deckType"] = "Constructed - Legacy"
        game["harnessEpoch"] = 3
        (games_dir / "game_20260101_000000.json.gz").write_bytes(gzip.compress(json.dumps(game).encode()))

        models_json = root / "models.json"
        models_json.write_text(json.dumps({"models": []}))

        output_path = generate_leaderboard_file(
            games_dir,
            data_dir,
            models_json,
        )
        result = json.loads(output_path.read_text())

        # Backward compat: top-level fields
        assert "totalGames" in result
        assert "models" in result
        assert result["totalGames"] == 1

        # New: per-format data
        assert "formats" in result
        assert "combined" in result["formats"]
        assert "legacy" in result["formats"]
        assert result["formats"]["legacy"]["totalGames"] == 1


# --- _player_key / _split_key ---


def test_player_key_without_effort():
    assert _player_key({"model": "a/x"}) == "a/x"


def test_player_key_with_effort():
    assert _player_key({"model": "a/x", "reasoningEffort": "medium"}) == "a/x::medium"


def test_player_key_with_snake_case_effort():
    assert _player_key({"model": "a/x", "reasoning_effort": "low"}) == "a/x::low"


def test_split_key_without_effort():
    assert _split_key("a/x") == ("a/x", None)


def test_split_key_with_effort():
    assert _split_key("a/x::medium") == ("a/x", "medium")


# --- reasoning effort in leaderboard ---


def test_generate_leaderboard_splits_by_reasoning_effort():
    """Same model at different effort levels should produce separate entries."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/haiku", cost=1.0, placement=1, reasoning_effort="low"),
                _pilot("Bob", "a/haiku", cost=2.0, placement=2, reasoning_effort="medium"),
            ],
        ),
    ]
    result, _ = generate_leaderboard(
        games,
        {"a/haiku": "Haiku"},
    )
    assert len(result["models"]) == 2
    names = {m["modelName"] for m in result["models"]}
    assert "Haiku (low)" in names
    assert "Haiku (medium)" in names

    # Both should have the same modelId
    for m in result["models"]:
        assert m["modelId"] == "a/haiku"

    # Winner should have reasoningEffort field
    winner = next(m for m in result["models"] if m["modelName"] == "Haiku (low)")
    assert winner["reasoningEffort"] == "low"


def test_generate_leaderboard_no_effort_no_suffix():
    """Players without reasoningEffort should have no suffix in display name."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/model-a", cost=1.0, placement=1),
                _pilot("Bob", "b/model-b", cost=2.0, placement=2),
            ],
        ),
    ]
    result, _ = generate_leaderboard(
        games,
        {"a/model-a": "Model A"},
    )
    model_a = next(m for m in result["models"] if m["modelId"] == "a/model-a")
    assert model_a["modelName"] == "Model A"
    assert "reasoningEffort" not in model_a


def test_generate_leaderboard_avg_blunders():
    """Average blunders per game should be computed from annotations."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/model-a", placement=1),
                _pilot("Bob", "b/model-b", placement=2),
            ],
        ),
        _make_game(
            "g2",
            "20260102_000000",
            "Bob",
            [
                _pilot("Alice", "a/model-a", placement=2),
                _pilot("Bob", "b/model-b", placement=1),
            ],
        ),
    ]
    games[0]["annotations"] = [
        {"type": "blunder", "player": "Alice", "severity": "major"},
        {"type": "blunder", "player": "Alice", "severity": "minor"},
        {"type": "blunder", "player": "Bob", "severity": "moderate"},
    ]
    games[1]["annotations"] = [
        {"type": "blunder", "player": "Bob", "severity": "major"},
    ]
    result, _ = generate_leaderboard(
        games,
        {},
    )

    alice = next(m for m in result["models"] if m["modelName"] == "Model A")
    bob = next(m for m in result["models"] if m["modelName"] == "Model B")

    # Alice: 2 blunders in game1, 0 in game2 -> 1.0 avg
    assert alice["avgBlundersPerGame"] == 1.0
    # Bob: 1 blunder in game1, 1 in game2 -> 1.0 avg
    assert bob["avgBlundersPerGame"] == 1.0


def test_generate_leaderboard_blunders_no_annotations():
    """Games without annotations should not count toward blunder average."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/model-a", placement=1), _pilot("Bob", "b/model-b", placement=2)],
        ),
    ]
    # No annotations key at all
    result, _ = generate_leaderboard(
        games,
        {},
    )

    alice = next(m for m in result["models"] if m["modelName"] == "Model A")
    assert alice["avgBlundersPerGame"] is None


def test_ratings_separate_by_effort():
    """Same model at different efforts should have independent ratings."""
    games = [
        _make_game(
            "g1",
            "20260101_000000",
            "Alice",
            [
                _pilot("Alice", "a/x", placement=1, reasoning_effort="medium"),
                _pilot("Bob", "a/x", placement=2, reasoning_effort="low"),
            ],
        ),
    ]
    ratings, _ = compute_ratings(games)
    assert "a/x::medium" in ratings
    assert "a/x::low" in ratings
    assert ratings["a/x::medium"] > ratings["a/x::low"]


# --- epoch filtering ---


def test_generate_leaderboard_file_excludes_old_epochs():
    """Games from old epochs should be excluded from ratings."""
    with tempfile.TemporaryDirectory() as tmpdir:
        root = Path(tmpdir)
        games_dir = root / "games"
        games_dir.mkdir()
        data_dir = root / "data"

        # Epoch 1 game (old, should be excluded)
        old_game = _make_game(
            "game_20260210_090000",
            "20260210_090000",
            "Alice",
            [_pilot("Alice", "a/x", placement=1), _pilot("Bob", "b/y", placement=2)],
        )
        old_game["deckType"] = "Constructed - Standard"
        (games_dir / "game_20260210_090000.json.gz").write_bytes(gzip.compress(json.dumps(old_game).encode()))

        # Epoch 4 game (current, should be included)
        new_game = _make_game(
            "game_20260215_090000",
            "20260215_090000",
            "Carol",
            [_pilot("Carol", "c/z", placement=1), _pilot("Dave", "d/w", placement=2)],
        )
        new_game["deckType"] = "Constructed - Standard"
        (games_dir / "game_20260215_090000.json.gz").write_bytes(gzip.compress(json.dumps(new_game).encode()))

        models_json = root / "models.json"
        models_json.write_text(json.dumps({"models": []}))

        output_path = generate_leaderboard_file(
            games_dir,
            data_dir,
            models_json,
        )
        result = json.loads(output_path.read_text())

        # Only the epoch 4 game should be in ratings (epoch 1 excluded, min is 2)
        assert result["totalGames"] == 1
        assert result["excludedGames"] == 1
        assert result["minEpoch"] == 2
        assert result["epochCounts"] == {"1": 1, "4": 1}

        # Only epoch-4 models should appear (epoch 1 is below MIN_LEADERBOARD_EPOCH=2)
        model_ids = {m["modelId"] for m in result["models"]}
        assert "c/z" in model_ids
        assert "a/x" not in model_ids


def test_generate_leaderboard_file_explicit_epoch_overrides_inferred():
    """Explicit harnessEpoch in game data should override timestamp inference."""
    with tempfile.TemporaryDirectory() as tmpdir:
        root = Path(tmpdir)
        games_dir = root / "games"
        games_dir.mkdir()
        data_dir = root / "data"

        # Old timestamp but explicit epoch 3 — should be included
        game = _make_game(
            "game_20260101_000000",
            "20260101_000000",
            "Alice",
            [_pilot("Alice", "a/x", placement=1), _pilot("Bob", "b/y", placement=2)],
        )
        game["deckType"] = "Constructed - Standard"
        game["harnessEpoch"] = 3
        (games_dir / "game_20260101_000000.json.gz").write_bytes(gzip.compress(json.dumps(game).encode()))

        models_json = root / "models.json"
        models_json.write_text(json.dumps({"models": []}))

        output_path = generate_leaderboard_file(
            games_dir,
            data_dir,
            models_json,
        )
        result = json.loads(output_path.read_text())

        assert result["totalGames"] == 1
        assert result["excludedGames"] == 0
